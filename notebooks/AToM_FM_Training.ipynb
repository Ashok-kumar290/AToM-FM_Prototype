{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AToM-FM: Adaptive Transformer of Multimodal Foundation Model\n",
    "## Qwen2.5 Fine-Tuning with QLoRA on RTX 4060 Ti\n",
    "\n",
    "This notebook provides a **complete interactive environment** for:\n",
    "1. Environment setup and GPU verification\n",
    "2. Model loading with 4-bit quantization (QLoRA)\n",
    "3. Dataset preparation and formatting\n",
    "4. Training with SFTTrainer\n",
    "5. Evaluation and inference\n",
    "6. Model export and merging\n",
    "\n",
    "**Hardware Target:** NVIDIA RTX 4060 Ti (8GB/16GB VRAM)  \n",
    "**Model:** Qwen/Qwen2.5-1.5B (default) with QLoRA adapters  \n",
    "**Dataset:** tatsu-lab/alpaca (52K instruction samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# Uncomment and run if needed:\n",
    "\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install transformers datasets accelerate peft trl bitsandbytes\n",
    "# !pip install wandb tensorboard evaluate sentencepiece\n",
    "# !pip install pyyaml omegaconf matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import trl\n",
    "import bitsandbytes\n",
    "\n",
    "print(f\"Python:        {sys.version}\")\n",
    "print(f\"PyTorch:       {torch.__version__}\")\n",
    "print(f\"Transformers:  {transformers.__version__}\")\n",
    "print(f\"PEFT:          {peft.__version__}\")\n",
    "print(f\"Datasets:      {datasets.__version__}\")\n",
    "print(f\"TRL:           {trl.__version__}\")\n",
    "print(f\"BitsAndBytes:  {bitsandbytes.__version__}\")\n",
    "print(f\"CUDA:          {torch.version.cuda}\")\n",
    "print(f\"GPU:           {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. GPU Verification & VRAM Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import print_gpu_info, print_vram_usage, set_seed, setup_logging\n",
    "\n",
    "setup_logging()\n",
    "set_seed(42)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"  GPU Information\")\n",
    "print(\"=\" * 50)\n",
    "gpu_info = print_gpu_info()\n",
    "print()\n",
    "print_vram_usage()\n",
    "\n",
    "# Determine recommended settings based on VRAM\n",
    "if gpu_info.get(\"vram_total_gb\", 0) >= 16:\n",
    "    print(\"\\n>> 16GB VRAM: Can use Qwen2.5-3B or even 7B with QLoRA\")\n",
    "elif gpu_info.get(\"vram_total_gb\", 0) >= 8:\n",
    "    print(\"\\n>> 8GB VRAM: Recommended Qwen2.5-1.5B with QLoRA (default config)\")\n",
    "else:\n",
    "    print(\"\\n>> <8GB VRAM: Use Qwen2.5-0.5B with QLoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Configuration\n",
    "\n",
    "You can either load from YAML files or define inline. We'll do both for flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_config\n",
    "\n",
    "# Option A: Load from YAML config files\n",
    "config = load_config(\"config\")\n",
    "\n",
    "# Option B: Override specific settings inline\n",
    "# Uncomment any of these to override:\n",
    "\n",
    "# config[\"model\"][\"name\"] = \"Qwen/Qwen2.5-0.5B\"     # smaller model for testing\n",
    "# config[\"model\"][\"name\"] = \"Qwen/Qwen2.5-3B\"        # larger model (needs 8GB+)\n",
    "# config[\"training\"][\"num_train_epochs\"] = 1           # quick test\n",
    "# config[\"training\"][\"max_steps\"] = 100                # very quick test\n",
    "# config[\"training\"][\"per_device_train_batch_size\"] = 1 # reduce if OOM\n",
    "# config[\"model\"][\"lora\"][\"r\"] = 32                    # reduce LoRA rank if OOM\n",
    "# config[\"model\"][\"tokenizer\"][\"max_length\"] = 1024    # reduce seq length if OOM\n",
    "\n",
    "print(\"Model:\", config[\"model\"][\"name\"])\n",
    "print(\"LoRA rank:\", config[\"model\"][\"lora\"][\"r\"])\n",
    "print(\"Epochs:\", config[\"training\"][\"num_train_epochs\"])\n",
    "print(\"Batch size:\", config[\"training\"][\"per_device_train_batch_size\"])\n",
    "print(\"Grad accum:\", config[\"training\"][\"gradient_accumulation_steps\"])\n",
    "print(\"Effective batch:\", config[\"training\"][\"per_device_train_batch_size\"] * config[\"training\"][\"gradient_accumulation_steps\"])\n",
    "print(\"Learning rate:\", config[\"training\"][\"learning_rate\"])\n",
    "print(\"Max seq length:\", config[\"sft\"][\"max_seq_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Load Model with QLoRA\n",
    "\n",
    "This loads the Qwen model in 4-bit precision and applies LoRA adapters.\n",
    "Only the LoRA parameters (~1-3% of total) are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import build_model_and_tokenizer, print_model_summary\n",
    "\n",
    "print(\"Loading model and tokenizer...\")\n",
    "print(f\"Model: {config['model']['name']}\")\n",
    "print(f\"Quantization: 4-bit NF4 with double quantization\")\n",
    "print(f\"LoRA rank: {config['model']['lora']['r']}\")\n",
    "print()\n",
    "\n",
    "model, tokenizer = build_model_and_tokenizer(config)\n",
    "\n",
    "print()\n",
    "print_model_summary(model)\n",
    "print()\n",
    "print_vram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect LoRA adapter layers\n",
    "print(\"LoRA adapter layers:\")\n",
    "print(\"=\" * 60)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"  {name:60s} | shape: {str(list(param.shape)):20s} | params: {param.numel():,}\")\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable: {trainable:,} / {total:,} = {100*trainable/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Dataset Preparation\n",
    "\n",
    "Default dataset: **tatsu-lab/alpaca** (52K instruction-following samples)  \n",
    "Format: `{instruction, input, output}` â†’ formatted prompt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import prepare_datasets, format_instruction\n",
    "\n",
    "print(f\"Loading dataset: {config['dataset']['name']}\")\n",
    "train_dataset, eval_dataset = prepare_datasets(config)\n",
    "\n",
    "print(f\"\\nTrain samples: {len(train_dataset):,}\")\n",
    "print(f\"Eval samples:  {len(eval_dataset):,}\")\n",
    "print(f\"Columns: {train_dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few formatted samples\n",
    "print(\"=\" * 60)\n",
    "print(\"Sample 1:\")\n",
    "print(\"=\" * 60)\n",
    "print(train_dataset[0][\"text\"][:800])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Sample 2:\")\n",
    "print(\"=\" * 60)\n",
    "print(train_dataset[1][\"text\"][:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token lengths to understand memory requirements\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample 1000 examples for length analysis\n",
    "sample_size = min(1000, len(train_dataset))\n",
    "sample_texts = [train_dataset[i][\"text\"] for i in range(sample_size)]\n",
    "token_lengths = [len(tokenizer.encode(t)) for t in sample_texts]\n",
    "\n",
    "print(f\"Token length statistics (sample of {sample_size}):\")\n",
    "print(f\"  Min:    {min(token_lengths)}\")\n",
    "print(f\"  Max:    {max(token_lengths)}\")\n",
    "print(f\"  Mean:   {np.mean(token_lengths):.0f}\")\n",
    "print(f\"  Median: {np.median(token_lengths):.0f}\")\n",
    "print(f\"  95th %%: {np.percentile(token_lengths, 95):.0f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(token_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(x=config['sft']['max_seq_length'], color='red', linestyle='--', label=f\"max_seq_length={config['sft']['max_seq_length']}\")\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Token Lengths in Training Data')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training\n",
    "\n",
    "Using `SFTTrainer` from TRL with:\n",
    "- QLoRA (4-bit base + LoRA adapters)\n",
    "- Gradient checkpointing (saves ~40% VRAM)\n",
    "- Paged AdamW 8-bit optimizer\n",
    "- NEFTune noise (improves instruction following)\n",
    "- Cosine LR schedule with warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.trainer import create_trainer\n",
    "\n",
    "trainer = create_trainer(model, tokenizer, train_dataset, eval_dataset, config)\n",
    "\n",
    "print(\"Trainer created!\")\n",
    "print(f\"  Total training steps: {trainer.state.max_steps if trainer.state.max_steps > 0 else 'auto'}\")\n",
    "print(f\"  Effective batch size: {config['training']['per_device_train_batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print()\n",
    "print_vram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# START TRAINING\n",
    "# ==========================================\n",
    "# This is the main training cell. On RTX 4060 Ti with Qwen2.5-1.5B:\n",
    "#   - ~3 epochs on Alpaca (52K samples) takes roughly 2-4 hours\n",
    "#   - VRAM usage: ~6-7 GB with default settings\n",
    "#   - If you get OOM, reduce batch_size to 1 or max_seq_length to 1024\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "print_vram_usage()\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Metrics: {train_result.metrics}\")\n",
    "print_vram_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_losses = [(h[\"step\"], h[\"loss\"]) for h in log_history if \"loss\" in h]\n",
    "eval_losses = [(h[\"step\"], h[\"eval_loss\"]) for h in log_history if \"eval_loss\" in h]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "if train_losses:\n",
    "    steps, losses = zip(*train_losses)\n",
    "    ax.plot(steps, losses, label=\"Train Loss\", alpha=0.7)\n",
    "\n",
    "if eval_losses:\n",
    "    steps, losses = zip(*eval_losses)\n",
    "    ax.plot(steps, losses, label=\"Eval Loss\", marker='o', markersize=4)\n",
    "\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"AToM-FM Training Progress\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned LoRA adapter\n",
    "SAVE_DIR = \"./models/final\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(f\"Model saved to: {SAVE_DIR}\")\n",
    "print(f\"Contents:\")\n",
    "for f in sorted(Path(SAVE_DIR).glob(\"*\")):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name:40s} {size_mb:8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the eval set\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in eval_metrics.items():\n",
    "    print(f\"  {key:30s}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Inference & Testing\n",
    "\n",
    "Test the fine-tuned model with various prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import generate_response\n",
    "\n",
    "def test_prompt(instruction, input_text=\"\", max_new_tokens=256):\n",
    "    \"\"\"Helper to test a prompt and display results.\"\"\"\n",
    "    print(f\"Instruction: {instruction}\")\n",
    "    if input_text:\n",
    "        print(f\"Input: {input_text}\")\n",
    "    print(\"-\" * 40)\n",
    "    response = generate_response(\n",
    "        model, tokenizer, instruction, input_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"=\" * 60)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Suite\n",
    "test_cases = [\n",
    "    {\n",
    "        \"instruction\": \"Explain what a neural network is in simple terms.\",\n",
    "        \"input\": \"\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Write a Python function to check if a string is a palindrome.\",\n",
    "        \"input\": \"\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Summarize the following text.\",\n",
    "        \"input\": \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Instead of being explicitly programmed, these systems use algorithms to identify patterns in data and make decisions with minimal human intervention. The field has seen tremendous growth in recent years, driven by the availability of large datasets and powerful computing resources.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Translate the following English text to French.\",\n",
    "        \"input\": \"The weather is beautiful today and I want to go for a walk in the park.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the three laws of thermodynamics? Explain each briefly.\",\n",
    "        \"input\": \"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  AToM-FM Test Suite\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "responses = []\n",
    "for i, tc in enumerate(test_cases):\n",
    "    print(f\"\\n--- Test {i+1}/{len(test_cases)} ---\")\n",
    "    resp = test_prompt(tc[\"instruction\"], tc.get(\"input\", \"\"))\n",
    "    responses.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Compare Base vs Fine-Tuned (Optional)\n",
    "\n",
    "Load the base model without LoRA to compare outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable LoRA adapter to get base model outputs\n",
    "model.disable_adapter_layers()\n",
    "\n",
    "prompt = \"Explain what transfer learning is and why it's useful.\"\n",
    "print(\"BASE MODEL (no LoRA):\")\n",
    "print(\"-\" * 40)\n",
    "base_response = generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7)\n",
    "print(base_response)\n",
    "\n",
    "# Re-enable LoRA\n",
    "model.enable_adapter_layers()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINE-TUNED MODEL (with LoRA):\")\n",
    "print(\"-\" * 40)\n",
    "ft_response = generate_response(model, tokenizer, prompt, max_new_tokens=200, temperature=0.7)\n",
    "print(ft_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Merge LoRA Weights (Optional)\n",
    "\n",
    "Merge the LoRA adapter into the base model for faster inference without PEFT dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: This requires more VRAM. Only run if you have enough memory.\n",
    "# For RTX 4060 Ti 8GB, this may OOM with 1.5B+ models.\n",
    "\n",
    "MERGE = False  # Set to True to merge\n",
    "\n",
    "if MERGE:\n",
    "    MERGED_DIR = \"./models/merged\"\n",
    "    os.makedirs(MERGED_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Merging LoRA weights into base model...\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "\n",
    "    print(f\"Saving merged model to {MERGED_DIR}...\")\n",
    "    merged_model.save_pretrained(MERGED_DIR)\n",
    "    tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "    print(\"Done! Merged model saved.\")\n",
    "    print(f\"Contents:\")\n",
    "    for f in sorted(Path(MERGED_DIR).glob(\"*\")):\n",
    "        size_mb = f.stat().st_size / 1e6\n",
    "        print(f\"  {f.name:40s} {size_mb:8.2f} MB\")\n",
    "else:\n",
    "    print(\"Skipping merge. Set MERGE=True above to merge LoRA into base model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. TensorBoard (Optional)\n",
    "\n",
    "View training metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard inline\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Custom Dataset Creation\n",
    "\n",
    "Use this section to create your own domain-specific dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset import create_custom_dataset\n",
    "\n",
    "# Example: Create a small custom dataset\n",
    "custom_instructions = [\n",
    "    \"What is AToM-FM?\",\n",
    "    \"Explain the AToM-FM architecture.\",\n",
    "    \"How does AToM-FM handle multimodal inputs?\",\n",
    "    \"What datasets can AToM-FM be trained on?\",\n",
    "    \"Compare AToM-FM with standard transformer models.\",\n",
    "]\n",
    "\n",
    "custom_outputs = [\n",
    "    \"AToM-FM (Adaptive Transformer of Multimodal Foundation Model) is a foundation model framework designed for adaptive learning across multiple modalities including text, code, and structured data.\",\n",
    "    \"AToM-FM uses a Qwen-based transformer backbone with QLoRA adapters for parameter-efficient fine-tuning. The architecture supports 4-bit quantization for deployment on consumer GPUs.\",\n",
    "    \"AToM-FM processes multimodal inputs through a unified tokenization scheme that maps different data modalities into a shared embedding space before passing them through the transformer layers.\",\n",
    "    \"AToM-FM can be trained on instruction-following datasets like Alpaca, OpenOrca, and domain-specific datasets. It also supports custom JSONL datasets with instruction/input/output format.\",\n",
    "    \"Unlike standard transformers that require full fine-tuning, AToM-FM uses QLoRA to train only 1-3% of parameters while maintaining comparable performance. This makes it accessible on consumer hardware like the RTX 4060 Ti.\",\n",
    "]\n",
    "\n",
    "custom_ds = create_custom_dataset(\n",
    "    instructions=custom_instructions,\n",
    "    outputs=custom_outputs,\n",
    "    save_path=\"./data/processed/custom_atom_fm.jsonl\",\n",
    ")\n",
    "\n",
    "print(f\"Custom dataset created: {len(custom_ds)} samples\")\n",
    "print(f\"Saved to: ./data/processed/custom_atom_fm.jsonl\")\n",
    "print(f\"\\nSample:\")\n",
    "print(custom_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Cleanup & Final VRAM Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final VRAM usage:\")\n",
    "print_vram_usage()\n",
    "\n",
    "# Uncomment to free GPU memory:\n",
    "# import gc\n",
    "# del model, trainer\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"\\nAfter cleanup:\")\n",
    "# print_vram_usage()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  AToM-FM Training Pipeline Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
